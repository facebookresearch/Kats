{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kats 204 Forecasting with Meta Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a time series and we are looking to build the best possible forecast (with respect to a predefined error metric such as mean absolute error) from the following list of candidate models (and possibly other forecasting models in Kats too):\n",
    "* ARIMA\n",
    "* SARIMA\n",
    "* Holt-Winters\n",
    "* Prophet\n",
    "* Theta\n",
    "* STLF\n",
    "\n",
    "\n",
    "\n",
    "For a single time series, it is straightforward to to do hyperparameter tuning for each of the candidate models with this time series, calculate the error metric, and choose the model that minimizes the error metric.  We have discussed this methodology in detail in Kats 201.  Our basic metadata object, `GetMetaData`, which we will introduce below, also does this calculation to find the best forecasting model for a single time series.\n",
    "\n",
    "However, when we are working with a large number of time series, repeating this process quickly becomes intractable, and for that, we include a meta-learning framework for forecasting.  There are two key model classes, plus one optional one, in our meta-learning framework:\n",
    "\n",
    "1. `MetaLearnModelSelect`: Given the metadata for a time series, predict the best model family (from the candidate models of interest) to forecast the series.  This model is a random forest by default.\n",
    "2. `MetaLearnHPT`: Given a time series and a model type, predict the best parameters for this model.  This model is a neutral network.  \n",
    "3. `MetaLearnPredictability` (optional): Given the metadata for a time series, predict if it is \"predictable\", i.e. if it is possible to forecast with a threshold error.  This model is a random forest by default.\n",
    "\n",
    "For each of these models, you can use labeled training data to build a model or you load a pre-trained model from a file path.  \n",
    "\n",
    "We use the `GetMetaData` object to represent the metadata for a time series in `MetaLearnModelSelect` and `MetaLearnPredictability`.  This tutorial begins with an introduction to the `GetMetaData` object.  Since this object is heavily dependent on `TsFeatures`, if you are not familiar `TsFeatures`, you should check out Kats 203 prior to continuing with this tutorial.  \n",
    "\n",
    "Next we will use labeled time series data from the `m3_meta_data.csv` file to show how to use the `MetaLearnPredictability`, `MetaLearnModelSelect` and `MetaLearnPredictability`.  \n",
    "\n",
    "\n",
    "The sample data in `m3_meta_data.csv` is very small, with 78 labeled examples, so the examples we provide here will not be highly accurate, but they will show you the proper workflow for using the meta-learning framework for forecasting in Kats.\n",
    "\n",
    "\n",
    "The full table of contents for Kats 204 is as follows:\n",
    "1. Introduction to `GetMetaData`\n",
    "2. Determining Predictability with `MetaLearnPredictability`\n",
    "3. Model Selection with `MetaLearnModelSelect`\n",
    "4. Hyperparameter Tuning with `MetaLearnHPT`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to `GetMetaData`\n",
    "\n",
    "The `GetMetaData` class generates the metadata for any time series.  There are three key components to the the metadata for a time series:\n",
    "1. `features`: the `TsFeatures` dictionary for the time series\n",
    "2. `hpt_res`: a dictionary giving the best hyperparameters for each candidate model and the corresponding error metric for the time series \n",
    "3. `best_model`: the name of the model with the smallest error metric\n",
    "\n",
    "The default error metric is mean absolute error (mae) but this can be controlled with the `error_method` argument in `GetMetaData`.  \n",
    "\n",
    "The list of candidate models that we consider is controlled by the `all_models` argument in `GetMetaData`, which is a dictionary with string names of the candidate models as keys and corresponding model classes as value.\n",
    "\n",
    "with keys equal to the string names of the models as keys  and values equal to the corresponding model class.  The keys in `hpt_res` and the value of `best_model` come from the keys of the `all_models` dictionary.  The default value of `all_models` will include the following six models.\n",
    "\n",
    "1. ARIMA\n",
    "2. SARIMA\n",
    "3. Holt-Winters\n",
    "4. Prophet`\n",
    "5. Theta\n",
    "6. STLF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first example uses the `air_passengers` data set.  We show how to get the metadata for this time series.  We start by loading the time series into a `TimeSeriesData` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore')\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from kats.consts import TimeSeriesData\n",
    "air_passengers_df = pd.read_csv(\"../kats/data/air_passengers.csv\")\n",
    "\n",
    "air_passengers_df.columns = [\"time\", \"value\"]\n",
    "air_passengers_ts = TimeSeriesData(air_passengers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can construct the `GetMetaData` object with all of the default settings for the `air_passengers` data set time series like follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kats.models.metalearner.get_metadata import GetMetaData\n",
    "\n",
    "# create an object MD of class GetMetaData\n",
    "MD = GetMetaData(data=air_passengers_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the `all_models` dictionary that is used by default here.  You are allowed to specify your own `all_models` dictionary as long as all the values are classes that extends the abstract class `kats.models.Model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arima': kats.models.arima.ARIMAModel,\n",
       " 'holtwinters': kats.models.holtwinters.HoltWintersModel,\n",
       " 'prophet': kats.models.prophet.ProphetModel,\n",
       " 'theta': kats.models.theta.ThetaModel,\n",
       " 'stlf': kats.models.stlf.STLFModel,\n",
       " 'sarima': kats.models.sarima.SARIMAModel}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MD.all_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `all_params` dictionary will have the same keys as the `all_models` dictionary, and the values are the corresponding parameter class (i.e. a class that extends the class `kats.const.Params`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arima': kats.models.arima.ARIMAParams,\n",
       " 'holtwinters': kats.models.holtwinters.HoltWintersParams,\n",
       " 'prophet': kats.models.prophet.ProphetParams,\n",
       " 'theta': kats.models.theta.ThetaParams,\n",
       " 'stlf': kats.models.stlf.STLFParams,\n",
       " 'sarima': kats.models.sarima.SARIMAParams}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MD.all_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `get_meta_data` function to calculate all the metadata and output the result as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Feature vector contains NAN, features are {'length': 144, 'mean': 0.45064085387638453, 'var': 0.03694123647244219, 'entropy': 0.4287365561752448, 'lumpiness': 2.0317879601323604e-05, 'stability': 0.03180185085604344, 'flat_spots': 2, 'hurst': -0.08023291030513345, 'std1st_der': 0.043740012626144645, 'crossing_points': 7, 'binarize_mean': 0.4444444444444444, 'unitroot_kpss': 0.1284750818014943, 'heterogeneity': 126.06450625819339, 'histogram_mode': 0.2504823151125402, 'linearity': 0.8536381656031872, 'trend_strength': 0.9383301875692747, 'seasonality_strength': 0.3299338017939567, 'spikiness': 7.462446542148283e-10, 'peak': 6, 'trough': 3, 'level_shift_idx': 118, 'level_shift_size': 0.025080385852090048, 'y_acf1': 0.9480473407524915, 'y_acf5': 3.392072131604335, 'diff1y_acf1': 0.3028552581521692, 'diff1y_acf5': 0.25945910659994703, 'diff2y_acf1': -0.19100586757092758, 'diff2y_acf5': 0.13420736423784568, 'y_pacf5': 1.003288249401527, 'diff1y_pacf5': 0.21941234780081384, 'diff2y_pacf5': 0.26101034286994845, 'seas_acf1': 0.6629043863684491, 'seas_pacf1': 0.156169552555893, 'firstmin_ac': 8, 'firstzero_ac': 52, 'holt_alpha': 0.9999999850988388, 'holt_beta': nan, 'hw_alpha': 0.9999999850988388, 'hw_beta': nan, 'hw_gamma': nan}.\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/base/model.py:568: ConvergenceWarning:\n",
      "\n",
      "Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/base/model.py:568: ConvergenceWarning:\n",
      "\n",
      "Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/base/model.py:568: ConvergenceWarning:\n",
      "\n",
      "Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/base/model.py:568: ConvergenceWarning:\n",
      "\n",
      "Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/base/model.py:568: ConvergenceWarning:\n",
      "\n",
      "Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/base/model.py:568: ConvergenceWarning:\n",
      "\n",
      "Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/base/model.py:548: HessianInversionWarning:\n",
      "\n",
      "Inverting hessian failed, no bse or cov_params available\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/base/model.py:568: ConvergenceWarning:\n",
      "\n",
      "Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "\n",
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "WARNING:fbprophet.models:Optimization terminated abnormally. Falling back to Newton.\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/base/model.py:548: HessianInversionWarning:\n",
      "\n",
      "Inverting hessian failed, no bse or cov_params available\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/base/model.py:568: ConvergenceWarning:\n",
      "\n",
      "Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "\n",
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/base/model.py:568: ConvergenceWarning:\n",
      "\n",
      "Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/base/model.py:568: ConvergenceWarning:\n",
      "\n",
      "Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "\n",
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/base/model.py:548: HessianInversionWarning:\n",
      "\n",
      "Inverting hessian failed, no bse or cov_params available\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/base/model.py:568: ConvergenceWarning:\n",
      "\n",
      "Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/base/model.py:568: ConvergenceWarning:\n",
      "\n",
      "Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "\n",
      "WARNING:fbprophet.models:Optimization terminated abnormally. Falling back to Newton.\n",
      "WARNING:fbprophet.models:Optimization terminated abnormally. Falling back to Newton.\n",
      "WARNING:fbprophet.models:Optimization terminated abnormally. Falling back to Newton.\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/base/model.py:568: ConvergenceWarning:\n",
      "\n",
      "Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "\n",
      "WARNING:fbprophet.models:Optimization terminated abnormally. Falling back to Newton.\n",
      "WARNING:fbprophet.models:Optimization terminated abnormally. Falling back to Newton.\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/base/model.py:548: HessianInversionWarning:\n",
      "\n",
      "Inverting hessian failed, no bse or cov_params available\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/base/model.py:568: ConvergenceWarning:\n",
      "\n",
      "Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/statsmodels/base/model.py:548: HessianInversionWarning:\n",
      "\n",
      "Inverting hessian failed, no bse or cov_params available\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/base/model.py:548: HessianInversionWarning:\n",
      "\n",
      "Inverting hessian failed, no bse or cov_params available\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/base/model.py:548: HessianInversionWarning:\n",
      "\n",
      "Inverting hessian failed, no bse or cov_params available\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/base/model.py:548: HessianInversionWarning:\n",
      "\n",
      "Inverting hessian failed, no bse or cov_params available\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/base/model.py:548: HessianInversionWarning:\n",
      "\n",
      "Inverting hessian failed, no bse or cov_params available\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/base/model.py:548: HessianInversionWarning:\n",
      "\n",
      "Inverting hessian failed, no bse or cov_params available\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/statsmodels/base/model.py:548: HessianInversionWarning:\n",
      "\n",
      "Inverting hessian failed, no bse or cov_params available\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get meta data as a dictionary\n",
    "air_passengers_metadata = MD.get_meta_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the keys of the metadata dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hpt_res', 'features', 'best_model', 'search_method', 'error_method'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_passengers_metadata.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explained what `features`, `hpt_res` and `best_model` are above.  This dictionary also includes the `search_method` and `error_method`, which will just be the default values in this case.  We can see these as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search_method: RandomSearch\n",
      "error_method: mae\n"
     ]
    }
   ],
   "source": [
    "print(f\"search_method: {air_passengers_metadata['search_method']}\")\n",
    "print(f\"error_method: {air_passengers_metadata['error_method']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keys of the `hpt_res` dictionary are name of the candidate model families; they should be the same as the keys for the `all_models` and `all_parameters` dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['arima', 'holtwinters', 'prophet', 'theta', 'stlf', 'sarima'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_passengers_metadata['hpt_res'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the `hpt_res` dictionary are two-element tuples.  The first element is gives the hyperparameters that minimize the error metric.  The second element gives the corresponding minimum error metric.  Let's take a look at these values for ARIMA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'p': 5, 'd': 1, 'q': 1}, 0.07938630340450795)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_passengers_metadata['hpt_res']['arima']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sort the different methods by their error metric as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prophet', 'stlf', 'holtwinters', 'theta', 'arima', 'sarima']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methods = list(air_passengers_metadata['hpt_res'].keys())\n",
    "sorted(methods, key = lambda m: air_passengers_metadata['hpt_res'][m][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that Prophet has the lowest error metric.  Let's confirm that this is what `best_model` indicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prophet'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_passengers_metadata['best_model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We constructed the `GetMetaData` object for the `air_passengers` data set with all of the default settings.  To wrap up this introduction to `GetMetaData`, let's take a look at the full set of attributes that can be used to initialize `GetMetadata`.\n",
    "\n",
    "This is the only required attributed: \n",
    "* **data**: TimeSeriesData, the time series for which we calculate the metadata\n",
    "\n",
    "The following attributes are all optional:\n",
    "* **all_models**: `Dict[str, m.Model]`, a dictionary for the candidate model classes.  The key is a string naming the model and each value is a corresponding model class (i.e. a class that extends the abstract class `kats.models.Model`).\n",
    "* **all_params**: `Dict[str, Params]`, a dictionary for the candidate model parameter classes.  The keys are the same as the keys for `all_models` and each value is a corresponding parameter class (i.e. a class that extends the class `kats.const.Params`).\n",
    "* **min_length**: int, the minimal length of time series. We raise a value error if the length of `data` is smaller than `min_length`.  The default value of `min_length` is 30.\n",
    "* **scale**: bool, Whether to rescale the time series by its maximum value; default is true.\n",
    "* **method**: SearchMethodEnum, Search method for hyper-parameters tuning; default is random search in the default parameter space\n",
    "* **executor**: Callable, A parallel executor for parallel processing.  By default, we use Python's native multiprocessing implementation.\n",
    "* **error_method**: str, Type of error metric.  Options are `'mape`', `'smape`',`'mae`', `'mase`', `'mse`', `'rmse`';  default is `'mae'`.\n",
    "* **num_trials**: int, Number of trials for hyperparameter search; default is 5\n",
    "* **num_arm**: optional Number of arms in hyperparameter search; default is 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Determining Predictability with `MetaLearnPredictability`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we pre-collected meta-data from m3 monthly data to demonstrate how to build and use meta-learning models. Notice that we are using a very small meta-data dataset (i.e., 78 records) and hence the performance of meta-learning models may not be ideal. In real applications, please considering using a relatively large meta-data dataset to ensure good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using meta-learning models for model and hyper-parameters forecasting, we would like to know whether a target time series can be easily fitted by a simple model or should be taken care of by extra human efforts. If a time series is taken as unpredictable, then the results given by our meta-learning framework may not be satisfying. Whether a time series is predictable or not is defined by user via specifying the \"threshold\" parameter for a target error metric (i.e., the model will take the time series with error metric>=threshold as unpredicable). This step is optional.\n",
    "\n",
    "Parameters for MetaLearnPredictability() class:\n",
    "* **metadata**: Optional; A list of dictionaries representing the meta-data of time series (e.g., the meta-data generated by GetMetaData object).Each dictionary d must contain at least 3 components: 'hpt_res', 'features' and 'best_model'. d['hpt_res'] represents the best hyper-parameters for each candidate model and the corresponding errors;\n",
    "d['features'] are time series features, and d['best_model'] is a string representing the best candidate model of the corresponding time series data. metadata should not be None unless load_model is True. Default is None\n",
    "\n",
    "* **threshold**: Optional; A float representing the threshold for the forecasting error. A time series whose forecasting error of the best forecasting model is higher than the threshold is considered as unpredictable. Default is 0.2.\n",
    "\n",
    "* **load_model**: Optional; A boolean to specify whether or not to load a trained model. Default is None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining examples, we use the sample data in `m3_meta_data.csv` to show how to build meta-learning models.  This sample data set contains the metadata for 78 time series.  While 78 metadata objects is certainly too few to develop an accurate meta-learning model and you should use more examples for your own meta-learning models to get high accuracy, these examples will help familiarize you with our meta-learning framework.\n",
    "\n",
    "We load the metadata from `m3_meta_data.csv` as follows and preview it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hpt_res</th>\n",
       "      <th>features</th>\n",
       "      <th>best_model</th>\n",
       "      <th>search_method</th>\n",
       "      <th>error_method</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'arima': ({'p': 5, 'd': 2, 'q': 5}, 0.8932924...</td>\n",
       "      <td>{'length': 68, 'mean': 0.35392156862745106, 'v...</td>\n",
       "      <td>stlf</td>\n",
       "      <td>RandomSearch</td>\n",
       "      <td>mape</td>\n",
       "      <td>N1402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'arima': ({'p': 5, 'd': 2, 'q': 1}, 0.3989958...</td>\n",
       "      <td>{'length': 68, 'mean': 0.19995256166982928, 'v...</td>\n",
       "      <td>arima</td>\n",
       "      <td>RandomSearch</td>\n",
       "      <td>mape</td>\n",
       "      <td>N1403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'arima': ({'p': 1, 'd': 2, 'q': 3}, 0.2519500...</td>\n",
       "      <td>{'length': 68, 'mean': 0.48962530217566486, 'v...</td>\n",
       "      <td>holtwinters</td>\n",
       "      <td>RandomSearch</td>\n",
       "      <td>mape</td>\n",
       "      <td>N1404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'arima': ({'p': 2, 'd': 1, 'q': 3}, 0.3146896...</td>\n",
       "      <td>{'length': 68, 'mean': 0.2980870396939263, 'va...</td>\n",
       "      <td>stlf</td>\n",
       "      <td>RandomSearch</td>\n",
       "      <td>mape</td>\n",
       "      <td>N1405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'arima': ({'p': 3, 'd': 1, 'q': 1}, 0.1448053...</td>\n",
       "      <td>{'length': 68, 'mean': 0.5336050082683677, 'va...</td>\n",
       "      <td>arima</td>\n",
       "      <td>RandomSearch</td>\n",
       "      <td>mape</td>\n",
       "      <td>N1406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             hpt_res  \\\n",
       "0  {'arima': ({'p': 5, 'd': 2, 'q': 5}, 0.8932924...   \n",
       "1  {'arima': ({'p': 5, 'd': 2, 'q': 1}, 0.3989958...   \n",
       "2  {'arima': ({'p': 1, 'd': 2, 'q': 3}, 0.2519500...   \n",
       "3  {'arima': ({'p': 2, 'd': 1, 'q': 3}, 0.3146896...   \n",
       "4  {'arima': ({'p': 3, 'd': 1, 'q': 1}, 0.1448053...   \n",
       "\n",
       "                                            features   best_model  \\\n",
       "0  {'length': 68, 'mean': 0.35392156862745106, 'v...         stlf   \n",
       "1  {'length': 68, 'mean': 0.19995256166982928, 'v...        arima   \n",
       "2  {'length': 68, 'mean': 0.48962530217566486, 'v...  holtwinters   \n",
       "3  {'length': 68, 'mean': 0.2980870396939263, 'va...         stlf   \n",
       "4  {'length': 68, 'mean': 0.5336050082683677, 'va...        arima   \n",
       "\n",
       "  search_method error_method    idx  \n",
       "0  RandomSearch         mape  N1402  \n",
       "1  RandomSearch         mape  N1403  \n",
       "2  RandomSearch         mape  N1404  \n",
       "3  RandomSearch         mape  N1405  \n",
       "4  RandomSearch         mape  N1406  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the metadata into a DataFrame\n",
    "meta_data_df = pd.read_csv(\"../kats/data/m3_meta_data.csv\")\n",
    "\n",
    "# We need to do a little pre-processing to make sure the dictionaries are represented as dictionaries\n",
    "# rather than as strings.  This function will do that pre-processing.\n",
    "def change_format(tmp):\n",
    "    tmp['hpt_res']=eval(tmp['hpt_res'])\n",
    "    tmp['hpt_res']['sarima'][0]['seasonal_order'] = eval(tmp['hpt_res']['sarima'][0]['seasonal_order'])\n",
    "    tmp['features']=eval(tmp['features'])\n",
    "    return tmp\n",
    "\n",
    "meta_data_df = meta_data_df.apply(change_format, axis=1)\n",
    "\n",
    "meta_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert this metadata DataFrame to a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = meta_data_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load class \n",
    "from kats.models.metalearner.metalearner_predictability import MetaLearnPredictability\n",
    "\n",
    "#take the time series with MAPE>=0.2 as unpreditable time series and initial the object\n",
    "mlp=MetaLearnPredictability(meta_data, threshold=0.2)\n",
    "\n",
    "#train a meta-learning predictability model and display the evaluation metrics on the test_set\n",
    "test_metrics=mlp.train()\n",
    "print(\"evaluation on test_set: \", test_metrics)\n",
    "\n",
    "#test whether TSdata is predictable or not\n",
    "val=mlp.pred(TSdata)\n",
    "val=(True if val==1 else False)\n",
    "print(\"TSdata is predictable: {}\".format(val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. **Meta-learn Model Selection**\n",
    "\n",
    "The MetaLearnModelSelect object helps to predict which predictive model is the most suitable one for the target time series.\n",
    "\n",
    "Parameters for MetaLearnModelSelect():\n",
    "* **metadata**: Optional; A list of dictionaries representing the meta-data of time series (e.g., the meta-data generated by GetMetaData object). Each dictionary d must contain at least 3 components: 'hpt_res', 'features' and 'best_model'. d['hpt_res'] represents the best hyper-parameters for each candidate model and the corresponding errors; d['features'] are time series features, and d['best_model'] is a string representing the best candidate model of the corresponding time series data. metadata should not be None unless load_model is True. Default is None\n",
    "* **load_model**: Optional; A boolean to specify whether or not to load a trained model. Default is False.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load MetaLearnModelSelect\n",
    "from kats.models.metalearner.metalearner_modelselect import MetaLearnModelSelect\n",
    "\n",
    "#create an object mlms of class MetaLearnModelSelect\n",
    "mlms = MetaLearnModelSelect(meta_data)\n",
    "\n",
    "#get class info\n",
    "counter=mlms.count_category()\n",
    "print(\"number of time series for each model: \", counter)\n",
    "\n",
    "#preprocessing meta-data: \n",
    "#not down-sample the meta-data to create the balanced data and standarized time series features to zero mean and unit standard deviation\n",
    "mlms.preprocess(downsample=False, scale=True)\n",
    "\n",
    "#generate features comparison plot\n",
    "mlms.plot_feature_comparison(10, 35)\n",
    "\n",
    "# generate heat map of correlation matrix of time series feature matrix\n",
    "mlms.plot_corr_heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meta-learning modelselect model is basically a multi-class classifier, and we currently support random foreset (default), GBDT, SVM, KNN, and naive Bayes. We also display the evaluation of the classifier in term of error metrics (i.e., MAPE for our example) on both the training set and test set, compared with the averaged error metric of using one model for all time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train a modelselect model using random forest algorithm   \n",
    "results=mlms.train()\n",
    "\n",
    "#display evaluation metrics of MetaLearnModelSelect\n",
    "summary=pd.DataFrame([results['fit_error'], results['pred_error']])\n",
    "summary['type']=['fit_error', 'pred_error']\n",
    "summary['error_metric']='MAPE'\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict forecasting model for a new time series data\n",
    "pred_model = mlms.pred(TSdata)\n",
    "print('Predicted forecasting model of this given time series data is: ', pred_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save trained model and load a pre-trained model\n",
    "mlms.save_model(\"mlms.pkl\")\n",
    "\n",
    "#initiate a new object and load the trained model\n",
    "mlms2 = MetaLearnModelSelect(load_model=True)\n",
    "mlms2.load_model(\"mlms.pkl\")\n",
    "\n",
    "#predict forecasting model with new MetaLearnModelSelect object\n",
    "pred_model = mlms2.pred(TSdata)\n",
    "print('Predicted forecasting model of this given time series data is: ', pred_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. **Meta-learning Hyper-parameter Selection**\n",
    "\n",
    "The MetaLearnHPT model generates the suitable hyper-parameters for a target time series given a designated model. We train a multi-task neural network using the optimal hyper-parameters from the meta-data and use it to predict the hyper-parameters.\n",
    "\n",
    "Some importand parameters for MetaLearnHPT():\n",
    "* **data_x**: Optional; A pd.DataFrame of time series features. data_x should not be None unless load_model is True. Default is None.\n",
    "* **data_y**: Optional; A pd.DataFrame of the corresponding best hyper-parameters. data_y should not be None unless load_model is True. Default is None.\n",
    "* **categorical_idx**: Optional; A list of strings of the names of the categorical hyper-parameters. Default is None.\n",
    "    If there is no categorical variable, then set categorical_idx as empty list.\n",
    "* **numerical_idx**: Optional; A list of strings of the names of the numerical hyper-parameters. Default is None.\n",
    "    If there is no numerical variables, then set numerical_idx as an empty list.\n",
    "* **default_model**: Optional; A string of the name of the forecast model whose default settings will be used. Can be 'arima', 'sarima', 'theta', 'prophet', 'holtwinters', 'stlf' or None. Default is None.\n",
    "    If None, then a customized model will be initiated.\n",
    "* **load_model**: Optional; A boolean to specify whether or not to load a trained model. Default is False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform meta-data into pd.DataFrame\n",
    "mlhpt_table={}\n",
    "for m in ['arima', 'sarima', 'stlf', 'theta', 'prophet', 'holtwinters']:\n",
    "    mlhpt_table[m]={'x':[], 'y':[]}\n",
    "    for elm in range(len(meta_data)):\n",
    "        mlhpt_table[m]['x'].append(meta_data[elm]['features'])\n",
    "        mlhpt_table[m]['y'].append(meta_data[elm]['hpt_res'][m][0])\n",
    "\n",
    "for tab in mlhpt_table:\n",
    "    mlhpt_table[tab]['x']=pd.DataFrame(mlhpt_table[tab]['x'])\n",
    "    mlhpt_table[tab]['y']=pd.DataFrame(mlhpt_table[tab]['y'])\n",
    "    \n",
    "#load MetaLearnHPT\n",
    "from kats.models.metalearner.metalearner_hpt import MetaLearnHPT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 MetaLearnHPT with default NNs\n",
    "\n",
    "For the users who want to avoid specifying the types of hyper-parameters and the neural network structures, we provide user-friendly default neural network structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an object using our default neural network (take Holt-Winter's model as an example)\n",
    "tab='holtwinters'\n",
    "mlhpt_holtwinters = MetaLearnHPT(\n",
    "    data_x=mlhpt_table[tab]['x'],\n",
    "    data_y=mlhpt_table[tab]['y'],\n",
    "    default_model=tab\n",
    ")\n",
    "\n",
    "#build a multi-task neural network using our default NN structure\n",
    "mlhpt_holtwinters.build_network()\n",
    "\n",
    "#train the multi-task NN\n",
    "mlhpt_holtwinters.train(lr=0.001, batch_size=20)\n",
    "\n",
    "#plot the training curves\n",
    "mlhpt_holtwinters.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict hyper-parameters using TimeSeriesData\n",
    "pred=mlhpt_holtwinters.pred(TSdata)\n",
    "print(\"predict hyper-parameters: \", pred['parameters'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the trained model\n",
    "mlhpt_holtwinters.save_model(\"mlhpt_hw.pkl\")\n",
    "\n",
    "#initiate a new object to load the trained model\n",
    "mlhpt2=MetaLearnHPT(load_model=True)\n",
    "mlhpt2.load_model(\"mlhpt_hw.pkl\")\n",
    "\n",
    "#get prediction using the new object\n",
    "pred=mlhpt2.pred(TSdata)\n",
    "print(\"predict hyper-parameters: \", pred['parameters'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 MetaLearnHPT with customized NNs\n",
    "\n",
    "We also give users the flexibility to specify their own NNs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an object with customized structures (take Holt-Winter's model as an example).\n",
    "mlhpt_holtwinters=MetaLearnHPT(\n",
    "    data_x=pd.DataFrame(mlhpt_table[tab]['x']),\n",
    "    data_y=pd.DataFrame(mlhpt_table[tab]['y']),\t\n",
    "    #specify the names of cateogrical label\n",
    "    categorical_idx = [\n",
    "                        \"trend\",\n",
    "                        \"damped\",\n",
    "                        \"seasonal\",\n",
    "                    ],\n",
    "    #specify the names of continuous label\n",
    "    numerical_idx = [\"seasonal_periods\"]\n",
    "    \n",
    ")\n",
    "\n",
    "#build the customized NN\n",
    "mlhpt_holtwinters.build_network(\n",
    "    #One shared one-layer NN with 50 neurons.\n",
    "    n_hidden_shared=[50],\n",
    "    #Each classification task has its own task-specific NN. In this example, \"trend\" and \"dampled\" both have a two-layer NN respectively\n",
    "    #and \"seasonal\" has a one-layer NN.\n",
    "    n_hidden_cat_combo=[[20, 10], [20, 10], [20]], \n",
    "    #One task-specific one-layer NN with 30 neurons for regression task.\n",
    "    n_hidden_num=[30]\n",
    ")\n",
    "\n",
    "#train the customized NN\n",
    "mlhpt_holtwinters.train(    \n",
    "    #loss_scale is used to balance 2 types of losses: cross-entropy for classification tasks and MSE for regression tasks\n",
    "    loss_scale=30,\n",
    "    #learning rate\n",
    "    lr=0.005,\n",
    "    n_epochs=2000,\n",
    "    batch_size=16,\n",
    "    #supports ADAM and SGD\n",
    "    method='SGD',\n",
    "    #momentum in SGD.\n",
    "    momentum=0,\n",
    "    #early stop option.\n",
    "    n_epochs_stop=50,)\n",
    "\n",
    "#plot the training curves\n",
    "mlhpt_holtwinters.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "mlhpt_holtwinters.pred(TSdata)\n",
    "print(\"predict hyper-parameters: \", pred['parameters'].iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "5b6e8fba36db23bc4d54e0302cd75fdd75c29d9edcbab68d6cfc74e7e4b30305"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
